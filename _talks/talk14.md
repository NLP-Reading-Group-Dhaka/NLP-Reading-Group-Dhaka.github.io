---
name: Tutorial 3 - Optimization in Deep Learning 
speakers:
  - Toshiba Zaman
  - Shahad Mahmud
  - Zannatul Naim Shanto 
date: 2021-01-30
categories:
  - Deep Learning
  - Tutorial
links:
  - name: Recording
    icon: video
    absolute_url: https://drive.google.com/file/d/1IznTNEl7UgDRnJXgGmcVFj2EXKGwD32k/view?usp=sharing 
  - name: Slides
    absolute_url: https://docs.google.com/presentation/d/1NHgDwVG0gNwt5tSwWkEVfw6qkXD_Ty-1q_YKyUIt-SM/edit?usp=sharing
  - name: Notebook
    absolute_url: 
---
1. Gradiend descent: batch, stochastic, mini-batch gradient descent 
2. Optimization algorithms: SGD, Momentum, Adagrad, Adadelta, Adam 
3. Notebook comparison of optimization algorithms
